╔═══════════════════════════════════════════════════════════════════════╗
║                      PHASE 2 - QUICK REFERENCE                        ║
║                   MPI Multi-Node Deployment Guide                     ║
╚═══════════════════════════════════════════════════════════════════════╝

┌─────────────────────────────────────────────────────────────────────┐
│ 📊 PHASE 1 RESULTS (COMPLETED)                                      │
└─────────────────────────────────────────────────────────────────────┘

✅ Throughput:     137,050,378 nums/sec
✅ Speedup:        62.3× (vs 2.2M sequential)
✅ Scaling:        55.6% efficiency (excellent!)
✅ Tested:         33,333,333 numbers in 243ms
✅ Errors:         0 (production-stable)
✅ Status:         READY FOR PHASE 2

┌─────────────────────────────────────────────────────────────────────┐
│ 🚀 PHASE 2 QUICK START                                              │
└─────────────────────────────────────────────────────────────────────┘

## Step 1: Build (2 min)
ssh nct01225@glogin1.bsc.es
cd /gpfs/projects/nct_352/nct01225/collatz/MNv3
module load impi/2021.9.0
bash build_mpi.sh

## Step 2: Test 2 Nodes (5 min)
sbatch slurm_mpi_2nodes.slurm
watch tail -50 collatz_mpi2_*.out

## Step 3: Run 10 Nodes (10 min)
sbatch slurm_mpi_10nodes.slurm
watch tail -50 collatz_mpi_*.out

┌─────────────────────────────────────────────────────────────────────┐
│ 📈 EXPECTED PERFORMANCE                                              │
└─────────────────────────────────────────────────────────────────────┘

Configuration     Throughput      Time (1B nums)    Speedup
─────────────────────────────────────────────────────────────
1 node (Phase 1)  137M/s          7.3 sec           62×
2 nodes           274M/s          3.6 sec           125×
5 nodes           685M/s          1.5 sec           311×
10 nodes ⭐       1.37B/s         0.73 sec          623×
20 nodes          2.74B/s         0.36 sec          1,245×
50 nodes          6.85B/s         0.15 sec          3,114×

┌─────────────────────────────────────────────────────────────────────┐
│ ✅ SUCCESS CRITERIA                                                  │
└─────────────────────────────────────────────────────────────────────┘

2-Node Test:
  [ ] Throughput >250M nums/sec
  [ ] Scaling ~2× single node
  [ ] No errors or crashes

10-Node Production:
  [ ] Throughput >1B nums/sec
  [ ] Near-linear scaling (>90%)
  [ ] 1 billion numbers verified

┌─────────────────────────────────────────────────────────────────────┐
│ 🐛 TROUBLESHOOTING                                                   │
└─────────────────────────────────────────────────────────────────────┘

## MPI module not found
module avail mpi
module load openmpi/4.1.4  # Alternative

## Build fails
export MPICXX=mpic++       # Use OpenMPI instead of Intel
bash build_mpi.sh

## Job stuck in queue
squeue -u nct01225
scontrol show job <JOBID>
# Try: edit .slurm file, change --qos=gp_debug

## Low scaling efficiency (<80%)
# Check: are all nodes getting work?
grep "Testing" collatz_mpi_*_rank*.txt
# Each rank should show similar tested counts

┌─────────────────────────────────────────────────────────────────────┐
│ 📊 MONITORING COMMANDS                                               │
└─────────────────────────────────────────────────────────────────────┘

# Check queue
squeue -u nct01225

# Watch live output
watch -n 1 'tail -50 collatz_mpi*.out'

# Check all output files
ls -lh collatz_mpi*

# View results
cat collatz_mpi_*.out | grep "Throughput"

# Check per-rank logs
ls *_rank*.txt

┌─────────────────────────────────────────────────────────────────────┐
│ 🎯 DECISION TREE AFTER PHASE 2                                      │
└─────────────────────────────────────────────────────────────────────┘

10-node result >1.3B nums/sec ✅
  ├─ Option A: Scale to 20-50 nodes (2-7B nums/sec) [SAFE]
  ├─ Option B: Attempt GPU (Phase 3) [AMBITIOUS]
  └─ Option C: Hybrid (keep GPP + try GPU) [BEST]

10-node result 800M-1.3B nums/sec ⚠️
  ├─ Option A: Debug scaling (check NUMA, network)
  └─ Option B: Still good! Stay on GPP, scale to 20 nodes

10-node result <800M nums/sec ❌
  └─ Debug: Check logs, network, MPI configuration

┌─────────────────────────────────────────────────────────────────────┐
│ 🔬 VALIDATION CHECKS                                                 │
└─────────────────────────────────────────────────────────────────────┘

## Verify linear scaling
# 2-node should be ~2× single node
# 10-node should be ~10× single node

## Check work distribution
grep "Tested:" collatz_mpi_*.out
# Each rank should test similar numbers

## Verify correctness
# Compare avg_steps, max_steps with Phase 1
# Should be in same ballpark

┌─────────────────────────────────────────────────────────────────────┐
│ 📈 SCALING BEYOND 10 NODES                                           │
└─────────────────────────────────────────────────────────────────────┘

## Create 20-node job
cp slurm_mpi_10nodes.slurm slurm_mpi_20nodes.slurm
# Edit: --nodes=20, --ntasks=20
# Edit: COUNT=2000000000  # 2B numbers
sbatch slurm_mpi_20nodes.slurm

## Create 50-node job (if ambitious!)
cp slurm_mpi_10nodes.slurm slurm_mpi_50nodes.slurm
# Edit: --nodes=50, --ntasks=50
# Edit: COUNT=5000000000  # 5B numbers
# Edit: --time=00:10:00  # Short time ok
sbatch slurm_mpi_50nodes.slurm

┌─────────────────────────────────────────────────────────────────────┐
│ 🏆 HACKATHON SUCCESS METRICS                                        │
└─────────────────────────────────────────────────────────────────────┘

Minimum Success:
  ✓ Phase 1: 137M nums/sec (1 node)
  ✓ Phase 2: >1B nums/sec (10 nodes)
  ✓ Verified: >1 billion numbers around 2^71

Target Success:
  ✓ Phase 2: >1.3B nums/sec (10 nodes, 90%+ efficiency)
  ✓ Verified: >10 billion numbers
  ✓ Demonstrated linear scaling

Outstanding Success:
  ✓ Phase 2: >2B nums/sec (20+ nodes)
  ✓ Or Phase 3: GPU working (>300M per GPU)
  ✓ Verified: >100 billion numbers
  ✓ Scientific analysis of results

┌─────────────────────────────────────────────────────────────────────┐
│ ⏱️ TIMELINE ESTIMATE                                                 │
└─────────────────────────────────────────────────────────────────────┘

Now:           Phase 1 complete ✅
+2 min:        Build Phase 2
+7 min:        2-node test completes ✅
+17 min:       10-node production completes ✅
---
Total:         20 minutes to 1.37B nums/sec!

Then (optional):
+10 min:       20-node run (2.74B nums/sec)
+10 min:       50-node run (6.85B nums/sec)
---
Or:            Start Phase 3 (GPU) development

┌─────────────────────────────────────────────────────────────────────┐
│ 💡 PRO TIPS                                                          │
└─────────────────────────────────────────────────────────────────────┘

1. ✅ Test 2 nodes FIRST (validation before scaling)
2. ✅ Monitor with 'watch' (live output is satisfying!)
3. ✅ Compare throughput per node (should be consistent)
4. ✅ Save output files (hackathon report evidence)
5. ✅ Take screenshots (for presentation)

┌─────────────────────────────────────────────────────────────────────┐
│ 🎓 KEY INSIGHTS                                                      │
└─────────────────────────────────────────────────────────────────────┘

Why Phase 2 Will Succeed:
  • Embarrassingly parallel (zero inter-node communication)
  • Phase 1 proven (137M/s single node is solid)
  • MPI just splits ranges (trivial work distribution)
  • Each node has full memo table (no data sharing)
  • Only sync: start barrier + final reduce

Why Linear Scaling Expected:
  • No communication overhead during computation
  • No shared memory contention
  • Equal work per node (balanced ranges)
  • Network only used at start/end

Potential Bottlenecks (unlikely):
  • Network congestion at MPI_Reduce (negligible data)
  • Queue wait time (use debug queue for 2-node test)
  • File system contention (each rank writes separate logs)

┌─────────────────────────────────────────────────────────────────────┐
│ 🚀 READY TO EXECUTE?                                                 │
└─────────────────────────────────────────────────────────────────────┘

Your next command:

  ssh nct01225@glogin1.bsc.es
  cd /gpfs/projects/nct_352/nct01225/collatz/MNv3
  bash build_mpi.sh
  sbatch slurm_mpi_2nodes.slurm

Expected result in 5 minutes:
  ✅ SUCCESS
  ✅ Throughput: ~274M nums/sec
  ✅ Verified: 100M numbers
  ✅ Ready for 10-node production

═══════════════════════════════════════════════════════════════════════

                    🎯 TARGET: 1 BILLION nums/sec
                    📊 EXPECTED: 1.37 BILLION nums/sec
                    🏆 CURRENT: On track for success!

═══════════════════════════════════════════════════════════════════════
