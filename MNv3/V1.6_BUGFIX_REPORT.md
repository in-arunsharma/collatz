# V1.6 → V1.6b Bug Fix Report

## Critical Bugs Found in V1.6-mpi-openmp.cpp

### Bug #1: Per-Rank Alignment (CATASTROPHIC)
**Location**: Work distribution section  
**Impact**: 54× slowdown, wrong test count (33M instead of 100M)

**Root Cause**:
```cpp
// WRONG (V1.6):
uint128_t my_start = start_offset + rank * count_per_rank;
align_start_and_delta(my_start, delta);  // Each rank aligns independently!
```

Each rank was calling `align_start_and_delta()` on its own starting point, which:
- Shifted each rank's range differently based on its mod-6 alignment
- Caused overlapping work between ranks
- Left gaps in coverage
- Resulted in wrong total count

**Fix (V1.6b)**:
```cpp
// CORRECT: Build GLOBAL list, then split by rank
uint128_t global_start = base + start_offset;
align_start_and_delta(global_start, delta);  // Align ONCE globally

std::vector<uint128_t> all_numbers;
{
    uint128_t n = global_start;
    uint64_t current_delta = delta;
    while (n < global_end) {
        all_numbers.push_back(n);
        n += current_delta;
        current_delta ^= 6;
    }
}

// Split by actual numbers (not by range)
size_t tests_per_rank = all_numbers.size() / size;
size_t my_start_idx = rank * tests_per_rank;
size_t my_end_idx = (rank == size - 1) ? total_tests : (rank + 1) * tests_per_rank;
```

### Bug #2: Range-Based Split (Not Count-Based)
**Impact**: Each rank got different number of actual tests

V1.6 split by **range** (e.g., rank 0 gets 0-50M, rank 1 gets 50M-100M), but mod-6 filtering means different ranges have different test counts!

V1.6b splits by **actual numbers to test** after filtering.

### Bug #3: Not a Bug (But Confusing Timing)
**Observation**: Precompute takes ~16 seconds per node

This is **correct** - each rank needs its own copy of the memo table. But it makes total runtime = precompute + compute, so Phase 2 appears slow in wall-clock time.

**Mitigation in V1.6b**:
- Reduced table to 2^19 (L2 cache friendly, faster precompute)
- Separated precompute time from compute time in reporting

## L2 Cache Optimization: 2^19 vs 2^20

### Your Question:
> "if we do the precomputed table to 2^19 instead of 2^20, it should fit in L2 cache. will it increase the throughput?"

### Analysis:

**Cache Hierarchy (MareNostrum 5 Sapphire Rapids)**:
- L1d per core: 48 KB
- L2 per core: 2 MB  ✅ **2^19 entries × 4 bytes = 2 MB**
- L3 shared: 105 MB (all cores share)

**2^20 table** (4 MB):
- Doesn't fit in L2 (2 MB per core)
- Must use L3 (shared by all 56 cores on socket)
- L3 bandwidth bottleneck with 112 threads hammering it

**2^19 table** (2 MB):
- Fits **perfectly** in L2 cache (private per core)
- Each thread gets dedicated L2 bandwidth (no contention)
- Fewer memo hits (half the range), but **faster** when hits occur

### Expected Impact:

**Phase 1 (single node)**: +10-20% throughput
- Reason: L2 private bandwidth >> L3 shared bandwidth
- Trade-off: 50% fewer memo hits, but 3× faster access
- Net: Likely **positive** because memory-bound workload

**Phase 2 (multi-node)**: +10-20% throughput per node
- Same reasoning, scales linearly

**Precompute time**: 50% faster
- 2^19 = half the entries = ~8 seconds instead of 16 seconds
- Faster node startup

### Recommendation:

**YES, switch to 2^19 for Phase 2!**

Reasons:
1. Faster precompute (8s vs 16s per node)
2. L2-private access eliminates L3 contention
3. Better for multi-node (each node independent)
4. Checking 2^71+ numbers, memo hits only first ~19 steps anyway

**Trade-off**: If checking smaller numbers (e.g., 2^40-2^50), 2^20 would be better. But for your target (2^71+), 2^19 is optimal.

## V1.6b Changes Summary

1. **Fixed work distribution**: Global alignment, then split by count
2. **Changed memo table**: 2^20 → 2^19 (L2 cache friendly)
3. **Separated timing**: Precompute vs compute clearly distinguished
4. **Better reporting**: Shows per-node throughput for Phase 2 validation

## Expected Performance (V1.6b)

**Phase 1 baseline** (V1.5-openmp, 2^20 table):
- 137M nums/sec on 1 node

**Phase 1 with 2^19 table**:
- Expected: 150-165M nums/sec (+10-20%)

**Phase 2 with bug fixes + 2^19**:
- Expected: 150M × N nodes (near-linear scaling)
- 2 nodes: ~300M nums/sec
- 10 nodes: ~1.5B nums/sec

## Action Items

1. **Build V1.6b**: `cd MNv3 && ./build_mpi.sh V1.6b-mpi-openmp.cpp`
2. **Test single node**: Verify it matches/beats V1.5 (137M+ nums/sec)
3. **Test 2 nodes**: Should get ~300M nums/sec (not 2.5M!)
4. **Scale to 10 nodes**: Should get ~1.5B nums/sec
5. **Compare 2^19 vs 2^20**: Run both, measure difference

## Verification Commands

```bash
# Build
cd /gpfs/projects/nct_352/nct01225/collatz/MNv3/
module load gcc openmpi
mpicxx -O3 -march=native -fopenmp -o V1.6b-mpi-openmp V1.6b-mpi-openmp.cpp

# Test 1 node (should match Phase 1)
cat > test_v16b_1node.slurm << 'EOF'
#!/bin/bash
#SBATCH --job-name=collatz_v16b_1node
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=112
#SBATCH --time=00:10:00
#SBATCH --qos=debug
#SBATCH --partition=gpp

export OMP_NUM_THREADS=112
export OMP_PROC_BIND=spread
export OMP_PLACES=cores

mpirun -np 1 ./V1.6b-mpi-openmp 0 100000000 --tag v16b_1node
EOF

# Test 2 nodes (should get ~300M nums/sec)
cat > test_v16b_2nodes.slurm << 'EOF'
#!/bin/bash
#SBATCH --job-name=collatz_v16b_2nodes
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=112
#SBATCH --time=00:10:00
#SBATCH --qos=debug
#SBATCH --partition=gpp

export OMP_NUM_THREADS=112
export OMP_PROC_BIND=spread
export OMP_PLACES=cores

mpirun -np 2 ./V1.6b-mpi-openmp 0 100000000 --tag v16b_2nodes
EOF

sbatch test_v16b_1node.slurm
sbatch test_v16b_2nodes.slurm
```

## Root Cause Explanation

Your Phase 2 failure was **NOT** a cache issue - it was a logic bug in work distribution.

Think of it like this:
- You want to test numbers {1, 5, 7, 11, 13, 17, ...} (mod 6 filtered)
- V1.6 said: "Rank 0 tests range 0-50, rank 1 tests range 50-100"
- Then each rank **independently** aligned to mod-6 within its range
- Result: Rank 0 tests {1, 5, 7, ...}, rank 1 tests {53, 55, 59, ...}
- But 53 is not in the original sequence! Overlap and gaps everywhere.

V1.6b says:
- Build the FULL sequence {1, 5, 7, 11, 13, 17, 19, 23, ...}
- Rank 0 gets first half: {1, 5, 7, 11, 13, ...}
- Rank 1 gets second half: {..., 17, 19, 23, ...}
- No overlap, no gaps, correct count

The 54× slowdown suggests ranks were testing overlapping numbers (wasted work) and the wrong count (33M instead of 100M) confirms the sequence was corrupted.

## Confidence Level

**Very High** - This is a classic embarrassingly-parallel work distribution bug. The fix is clean and matches your successful V1.5 design.

Expected result: Phase 2 will now **work correctly** with near-linear scaling.
