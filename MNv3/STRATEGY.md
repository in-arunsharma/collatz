# ğŸ¯ MareNostrum 5 Collatz Scaling Strategy - EXECUTIVE SUMMARY

**Date:** October 14, 2025  
**Hackathon:** MareNostrum 5  
**Baseline:** V1.4b @ 2.2M nums/sec (sequential)  
**Goal:** Maximum throughput for Collatz conjecture verification

---

## âœ… RECOMMENDED STRATEGY: Start with GPP, then decide

### Phase 1: OpenMP Single-Node GPP â­ **START HERE** (30 min)
- **Status:** âœ… **READY TO DEPLOY**
- **Target:** 1 node Ã— 112 cores
- **Expected:** 150-200M nums/sec (70-90Ã— speedup)
- **Risk:** LOW âœ…
- **Files:** `MNv3/` folder ready
- **Action:** Deploy now, validate in 30 minutes

### Phase 2: MPI Multi-Node GPP (2 hours)
- **Status:** â³ Create after Phase 1 success
- **Target:** 10 nodes Ã— 112 cores = 1,120 cores
- **Expected:** 1-2 BILLION nums/sec (700Ã— speedup)
- **Risk:** LOW âœ…
- **When:** If Phase 1 achieves >100M nums/sec

### Phase 3: CUDA Single-GPU ACC (3-4 hours)
- **Status:** â³ Alternative to Phase 2
- **Target:** 1 NVIDIA Hopper GPU
- **Expected:** 200-500M nums/sec per GPU
- **Risk:** MEDIUM âš ï¸ (new CUDA code)
- **When:** If you want maximum single-node performance

### Phase 4: Multi-GPU + MPI Hybrid (1 day)
- **Status:** ğŸ¯ Final target for maximum throughput
- **Target:** 50-100 nodes = 200-400 GPUs
- **Expected:** 10-50 BILLION nums/sec
- **Risk:** MEDIUM-HIGH âš ï¸
- **When:** After Phase 3 validation

---

## ğŸ“Š PERFORMANCE COMPARISON

| Phase | Hardware | Throughput | Speedup | Time to Deploy |
|-------|----------|------------|---------|----------------|
| V1.4b | 1 CPU core | 2.2M/s | 1Ã— | - |
| **Phase 1** | **1 GPP node** | **150M/s** | **70Ã—** | **30 min** â­ |
| Phase 2 | 10 GPP nodes | 1.5B/s | 700Ã— | 2 hours |
| Phase 3 | 1 GPU | 300M/s | 140Ã— | 3 hours |
| Phase 4 | 100 GPUs | 30B/s | 14,000Ã— | 1 day |

---

## ğŸš€ IMMEDIATE ACTION PLAN

### Right Now (5 minutes)
```bash
cd /home/aruns/Desktop/MN25
scp -r MNv3/ nct01225@glogin1.bsc.es:/gpfs/projects/nct_352/nct01225/collatz/
```

### On MareNostrum (10 minutes)
```bash
ssh nct01225@glogin1.bsc.es
cd /gpfs/projects/nct_352/nct01225/collatz/MNv3

# Test build
bash build_openmp.sh

# Quick validation
./V1.5-openmp 0 1000000 --threads 4 --tag login_test

# Submit 1-node job
sbatch slurm_openmp_1node.slurm
```

### Monitor (15 minutes)
```bash
# Check queue
squeue -u nct01225

# Watch output
watch tail -30 collatz_omp_*.out
```

---

## ğŸ¯ DECISION TREE

```
Phase 1 Result
â”‚
â”œâ”€ >150M nums/sec âœ…
â”‚  â”œâ”€ Option A: Phase 2 (MPI) â†’ 1.5B nums/sec [SAFE]
â”‚  â””â”€ Option B: Phase 3 (GPU) â†’ 300M nums/sec [AMBITIOUS]
â”‚
â”œâ”€ 100-150M nums/sec âš ï¸
â”‚  â”œâ”€ Option A: Optimize Phase 1, then MPI
â”‚  â””â”€ Option B: Jump to GPU (better single-node performance)
â”‚
â””â”€ <100M nums/sec âŒ
   â””â”€ Debug: Check NUMA, thread binding, profiling
```

---

## ğŸ’¡ WHY START WITH GPP?

### âœ… Advantages
1. **Fast deployment** (30 min vs 3 hours for GPU)
2. **Low risk** (OpenMP is simple, stable)
3. **Validates parallelization** (test before GPU complexity)
4. **Guaranteed resources** (less queue competition)
5. **Foundation for MPI** (same code, add MPI wrapper)
6. **Good performance** (150M nums/sec is respectable)

### âš ï¸ Why Not GPU First?
1. **Higher development time** (3-4 hours)
2. **More complex** (CUDA kernels, memory management)
3. **Higher risk** (bugs harder to debug)
4. **Queue competition** (ACC partition may be busy)
5. **Overkill for hackathon?** (GPP might be sufficient)

### ğŸ¯ When to Choose GPU?
- You need >1B nums/sec immediately
- You have CUDA experience
- You have 3+ hours available
- GPP results are disappointing (<100M nums/sec)

---

## ğŸ“ FILES READY FOR DEPLOYMENT

```
MNv3/
â”œâ”€â”€ V1.5-openmp.cpp              OpenMP parallelized version
â”œâ”€â”€ build_openmp.sh              Build script (g++ or icpx)
â”œâ”€â”€ slurm_openmp_1node.slurm     SLURM job (1 node, 112 cores)
â”œâ”€â”€ test_local.sh                Local testing (4 threads)
â”œâ”€â”€ README.md                    Quick reference
â”œâ”€â”€ DEPLOYMENT_GUIDE.md          Detailed step-by-step
â””â”€â”€ PHASE1_COMPLETE.md           Status report
```

**Status:** âœ… All files created and tested locally

---

## ğŸ”¬ TECHNICAL HIGHLIGHTS

### V1.5-openmp Features
- **Thread-safe:** Read-only memo table after precompute
- **Load-balanced:** Dynamic scheduling (512-number chunks)
- **Minimal overhead:** Thread-local statistics and cold queues
- **Edge cases:** Preserved overflow and cycle detection
- **Scalable:** No synchronization in hot path

### Optimization Flags
```bash
# GCC
-O3 -march=native -mtune=native -fopenmp -ffast-math

# Intel (recommended on MareNostrum)
-O3 -xHost -qopenmp -ipo -no-prec-div -fp-model fast=2
```

### SLURM Configuration
```bash
--nodes=1                   # Single node
--cpus-per-task=112         # All 112 cores
--time=02:00:00             # 2 hours (plenty)
--qos=gp_debug              # Fast queue
```

---

## â±ï¸ TIMELINE TO SUCCESS

```
T+0:00    Deploy MNv3 to MareNostrum
T+0:10    Build and test on login node
T+0:15    Submit 1-node job
T+0:20    Job starts (debug queue)
T+0:21    Job completes (100M numbers in ~1 min)
T+0:30    Analyze results, decide next phase
---
Phase 1:  30 minutes total

T+0:30    Start Phase 2 (MPI) development
T+1:30    Submit 2-node test
T+2:00    Submit 10-node production run
T+2:30    Achieve 1.5B nums/sec
---
Phase 2:  2.5 hours total from start

OR

T+0:30    Start Phase 3 (CUDA) development
T+2:00    Test single GPU kernel
T+3:00    Submit 1-GPU job
T+3:30    Achieve 300M nums/sec per GPU
T+4:00    Start multi-GPU (Phase 4)
---
Phase 3:  4 hours total from start
```

---

## âœ… SUCCESS CRITERIA

### Phase 1 (Minimum)
- [ ] Job completes without errors
- [ ] Throughput >100M nums/sec
- [ ] All edge cases logged correctly
- [ ] Results validate against V1.4b

### Phase 1 (Target)
- [ ] Throughput >150M nums/sec
- [ ] Speedup >70Ã—
- [ ] Scaling efficiency >60%
- [ ] Ready for Phase 2

### Overall Hackathon (Minimum)
- [ ] Verify 10 billion numbers
- [ ] Throughput >500M nums/sec sustained
- [ ] Complete scientific report

### Overall Hackathon (Target)
- [ ] Verify 100 billion numbers
- [ ] Throughput >1B nums/sec sustained
- [ ] Demonstrate GPU scaling

---

## ğŸ†˜ TROUBLESHOOTING QUICK REFERENCE

### Job won't start
```bash
squeue -u nct01225           # Check queue
sacctmgr show assoc user=$USER  # Check account
# Try: --qos=gp_bsccs instead of gp_debug
```

### Low performance
```bash
export OMP_PROC_BIND=close   # Thread pinning
export OMP_PLACES=cores      # NUMA awareness
numactl --cpunodebind=0 ./V1.5-openmp ...  # Force NUMA node
```

### Build errors
```bash
module load intel/2023.2.0   # Use Intel compiler
g++ --version                # Check version (need 9.0+)
```

---

## ğŸ“ SUPPORT

- **MareNostrum Docs:** https://www.bsc.es/user-support/mn5.php
- **Hackathon Mentors:** Ask during sessions
- **BSC Support:** support@bsc.es

---

## ğŸ“ KEY INSIGHTS

1. **Start simple, scale incrementally** (OpenMP â†’ MPI â†’ GPU)
2. **Validate each phase** (don't skip ahead without proof)
3. **Embarrassingly parallel = HPC paradise** (zero communication overhead)
4. **Read-only data = zero synchronization** (perfect scaling)
5. **Dynamic scheduling = automatic load balancing** (handles imbalanced work)

---

## ğŸš€ FINAL RECOMMENDATION

**FOR HACKATHON SUCCESS:**

1. âœ… **Deploy Phase 1 NOW** (30 minutes to validation)
2. â° **Wait for results** (likely >150M nums/sec)
3. ğŸš€ **Proceed to Phase 2** (MPI multi-node â†’ 1.5B nums/sec)
4. ğŸ¯ **Done!** (1.5B nums/sec is excellent for hackathon)

**FOR MAXIMUM PERFORMANCE:**

1. âœ… **Deploy Phase 1** (validate parallelization works)
2. ğŸ® **Jump to Phase 3** (GPU development in parallel)
3. ğŸ’ª **Scale to Phase 4** (100 GPUs â†’ 30B nums/sec)
4. ğŸ† **Win hackathon!** (Highest throughput possible)

---

**STATUS: ğŸŸ¢ READY TO DEPLOY**

**Next command:**
```bash
cd /home/aruns/Desktop/MN25
scp -r MNv3/ nct01225@glogin1.bsc.es:/gpfs/projects/nct_352/nct01225/collatz/
```

**Good luck! ğŸš€**
