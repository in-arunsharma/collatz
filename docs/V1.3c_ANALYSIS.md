# V1.3c Analysis: Thread-Safe Precomputed Memo

## Executive Summary

**V1.3c** implements a precomputed, read-only memo table optimized for parallel execution on OpenMP and CUDA. While 8% slower than V1.3a in single-threaded mode (1.75M vs 1.93M nums/sec), it provides **thread-safe** memo access essential for multi-core and GPU scaling.

**Key Tradeoff:** Sacrifice 8% sequential speed to enable 10-1000√ó parallel speedup.

---

## Architecture Changes

### V1.3a: Lazy Fill (Fast Sequential, Not Thread-Safe)
```
1. Allocate empty memo table (all UNKNOWN)
2. Start timing
3. For each number:
   - Check memo ‚Üí hit or miss
   - If miss: compute + write result + path compression fill
   - Continue
```

**Benefits:**
- Fills table during compute (caches partial results)
- Path compression backfills intermediate values
- Fewer full trajectory computations

**Limitations:**
- **NOT thread-safe** (concurrent writes to memo table)
- Requires locking in OpenMP (slow)
- Cannot use in CUDA without atomic operations (very slow)

### V1.3c: Precomputed Read-Only (Slower Sequential, Thread-Safe)
```
1. Allocate memo table
2. Precompute: Fill ALL 2^20 entries with path compression
   (NOT timed - one-time cost, can save to disk)
3. Start timing
4. For each number:
   - Check memo ‚Üí read-only lookup
   - If miss: compute (no writes to memo)
   - Continue
```

**Benefits:**
- **Thread-safe** - no writes during compute
- Trivial to parallelize (no locks needed)
- Predictable cache behavior
- Can save/load from disk (`steps_2p20.bin`)

**Limitations:**
- No lazy fill benefits during compute
- Read-only = can't cache new intermediate results
- 8% slower than V1.3a sequentially

---

## Performance Comparison

| Metric | V1.3a (Lazy Fill) | V1.3c (Precomputed) | Œî |
|--------|-------------------|---------------------|---|
| **Throughput** | 1,893,937 nums/sec | 1,749,533 nums/sec | **-8%** ‚ö†Ô∏è |
| **Time** | 176ms | 190ms | +14ms |
| **Core Instructions** | 2.65B | 3.07B | **+16%** |
| **Core Cycles** | 742M | 788M | +6% |
| **IPC (core)** | 4.89 | 4.72 | -3% |
| **Retiring** | 60.2% | 65.1% | +5% |
| **Branch Misses** | 462K (0.34%) | 360K (0.25%) | -22% ‚úÖ |
| **L1D Misses** | 59K | 33K | -44% ‚úÖ |
| **Thread-Safe?** | ‚ùå No | ‚úÖ **Yes** | **CRITICAL** |

**Analysis:**
- V1.3c has **16% more instructions** but better cache/branch behavior
- Higher instruction count from computing full trajectories (no intermediate caching)
- Better cache hits from preloaded table
- Fewer branch misses from stable memo access pattern
- **Thread-safety is the killer feature** for MareNostrum 5

---

## Why V1.3c is Slower (But Essential)

### Lazy Fill Advantage (V1.3a)
Consider trajectory: 100 ‚Üí 50 ‚Üí 25 ‚Üí 76 ‚Üí 38 ‚Üí 19 ‚Üí ...

1. Compute 100:
   - 100 not in memo
   - Compute full trajectory: 100 ‚Üí 50 ‚Üí 25 ‚Üí 76 ‚Üí 38 ‚Üí 19 ‚Üí ... ‚Üí 1
   - **Write to memo:** `memo[100] = 25, memo[50] = 24, memo[25] = 23, ...`
   - Path compression fills intermediate values

2. Later compute 50:
   - **memo[50] already filled!** ‚Üí instant return
   - Saved computing 24 steps

### Read-Only Limitation (V1.3c)
Same trajectory:

1. Precompute phase:
   - Fill `memo[1..2^20]` with all known values
   - 100, 50, 25, 76, 38, 19 all within 2^20 ‚Üí cached

2. Compute 100:
   - 100 < 2^20 ‚Üí **memo[100] cached** ‚Üí instant return ‚úÖ

3. Compute large number (e.g., 2^71 + 123):
   - 2^71 + 123 > 2^20 ‚Üí not in memo
   - Compute trajectory ‚Üí hits intermediate value 456
   - If 456 < 2^20 ‚Üí **memo[456] hit** ‚úÖ
   - If 456 > 2^20 ‚Üí **compute full path** ‚ö†Ô∏è (no caching)

**Key Insight:** V1.3c can't cache values > 2^20 discovered during compute. For 2^71 numbers, most trajectories involve large values, so we compute more full paths.

---

## Parallelization Strategy

### Why V1.3c Enables Scaling

**OpenMP (V1.4 - Coming Next):**
```cpp
#pragma omp parallel for
for (uint64_t i = 0; i < num_threads; i++) {
    // Each thread reads from memo (no locks!)
    // Process disjoint range of seeds
}
```

**CUDA (V1.5 - GPU):**
```cuda
__global__ void collatz_kernel(uint32_t* memo, ...) {
    // Copy memo to GPU constant/global memory
    // Each thread: read-only access (no synchronization!)
    // Massive parallelism: 1000s of threads
}
```

**MPI (V1.6 - Multi-Node):**
```cpp
// Each node gets precomputed memo table
// Partition seed range across nodes
// No communication during compute (embarrassingly parallel)
// Gather results at end
```

### Expected Speedups

| Platform | V1.3a (Sequential) | V1.3c (Parallel) | Speedup |
|----------|-------------------|------------------|---------|
| **Single Thread** | 1.93M nums/sec | 1.75M nums/sec ‚úÖ | 0.91√ó (baseline) |
| **80 Cores (MN5 CPU)** | N/A (not thread-safe) | ~**120M nums/sec** üöÄ | **70√ó** expected |
| **1 GPU (NVIDIA Hopper)** | N/A | ~**1-5 BILLION nums/sec** üöÄ | **1000-3000√ó** |
| **4,480 GPUs (MN5)** | N/A | ~**TRILLIONS/sec** üöÄ | **Million√ó** |

**Reality Check:** Expect 50-70% of theoretical max due to:
- Memory bandwidth limits
- Large number handling (uint128_t)
- Trajectory length variance
- GPU occupancy

**Conservative Estimates:**
- 80 cores: 50M nums/sec (30√ó speedup)
- 1 GPU: 500M nums/sec (300√ó speedup)
- Full MN5: Push 2^71 limit in **minutes** instead of years

---

## Disk Save/Load Feature

### Motivation
Precomputing 2^20 entries takes ~30ms. For benchmarking or production:
- Save once: `./V1.3c 0 1000000 --save steps_2p20.bin` (30ms overhead)
- Load many: `./V1.3c 0 1000000 --load steps_2p20.bin` (<1ms load)

### File Format
```
Header:
  uint32_t version = 1
  uint32_t small_limit_bits = 20

Data:
  uint32_t memo[2^20]  // 4MB for 2^20 entries
```

**Benefits:**
- Amortize precompute cost across runs
- Fast startup for benchmarking
- Consistent memo state for reproducibility

**Usage:**
```bash
# First run: precompute and save
./V1.3c 0 1000000 --save steps_2p20.bin
# [PRECOMPUTE] Filled 1048576 / 1048576 entries (100.00%)
# [SAVE] Wrote 1048576 entries to steps_2p20.bin

# Subsequent runs: load from disk
./V1.3c 0 1000000 --load steps_2p20.bin
# [LOAD] Loaded 1048576 entries from steps_2p20.bin
# (no precompute overhead!)

# Larger table size (16MB)
./V1.3c 0 1000000 --small-limit 22 --save steps_2p22.bin
```

---

## Engineering Lessons

### 1. Thread-Safety is Non-Negotiable for HPC
- Lazy fill (V1.3a): Fast but requires locks in parallel ‚Üí kills scalability
- Read-only (V1.3c): Slightly slower but trivial to parallelize ‚Üí enables GPUs

**Lesson:** For massively parallel systems (MareNostrum 5), thread-safety > 8% sequential speed.

### 2. Precompute Amortization
- Precompute cost: ~30ms for 2^20 entries
- Per-run benefit: 190ms compute time
- Amortization: After 1 run, cost negligible
- With disk save/load: <1ms load time

**Lesson:** One-time precompute is acceptable when amortized across parallel runs or saved to disk.

### 3. Cache Size Still Matters
- 2^20 (4MB): Fits L3 cache, good performance
- 2^22 (16MB): Spills to RAM, slower lookups
- 2^24 (64MB): Heavy cache pressure, not recommended

**Lesson:** Stick with 2^20 for best balance of coverage and cache efficiency.

### 4. Instruction Count ‚â† Performance
- V1.3c: +16% instructions but better cache/branch behavior
- IPC nearly the same (4.72 vs 4.89)
- Memory access patterns matter more than raw instruction count

**Lesson:** Optimize for memory hierarchy, not just instruction count.

---

## Recommendations

### For Sequential Benchmarking
**Use V1.3a** - Best sequential performance (1.93M nums/sec)

### For OpenMP/CUDA Development
**Use V1.3c** - Thread-safe foundation, enables parallelism

### For MareNostrum 5 Hackathon (Monday!)
**Priority:**
1. ‚úÖ V1.3c baseline (complete)
2. ‚Üí V1.4 OpenMP (multicore scaling proof)
3. ‚Üí V1.5 CUDA basics (GPU demo for judges)
4. ‚Üí V1.6 MPI (multi-node if time permits)

### Table Size Selection
- **2^20 (4MB):** Default, best balance ‚úÖ
- **2^22 (16MB):** If 90%+ trajectories hit small values
- **2^18 (1MB):** If memory-constrained (embedded)

---

## Next Steps: V1.4 OpenMP

**Goal:** Multicore CPU parallelization for 80-core MareNostrum 5 nodes

**Implementation Strategy:**
```cpp
#include <omp.h>

// Precompute once (shared)
precompute_small_table();

// Parallel compute
#pragma omp parallel for reduction(+:total_steps) schedule(dynamic, 1024)
for (uint64_t i = 0; i < num_seeds; i++) {
    uint128_t n = compute_seed(i);
    CollatzResult res = compute_collatz_readonly(n);  // Thread-safe!
    // Aggregate results
}
```

**Expected:**
- 50-70√ó speedup on 80 cores (accounting for overhead)
- Proof that V1.3c's thread-safety pays off
- Foundation for V1.5 (GPU) with same read-only pattern

---

## Conclusion

**V1.3c achieves its design goal:** Provide a thread-safe, precomputed memo table for massive parallelization, accepting an 8% sequential slowdown as necessary cost.

**Performance:**
- Sequential: 1.75M nums/sec (competitive)
- Parallel potential: 50-1000√ó speedup on MareNostrum 5
- Thread-safe: ‚úÖ Ready for OpenMP/CUDA/MPI

**Status:** ‚úÖ Ready for V1.4 (OpenMP) and V1.5 (CUDA) implementation

**For Monday's hackathon:** V1.3c is the foundation. Build parallel versions on top of this rock-solid, thread-safe base.

**Key Insight:** Sometimes the best optimization is making the code trivially parallelizable, even if it costs a few percent sequentially. The 1000√ó GPU speedup more than justifies the 8% sequential tradeoff.

üöÄ **Next:** Parallelize and conquer MareNostrum 5!
